{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     NCTNumber                          LeadSponsorName  \\\n",
      "0  NCT06036238  University of California, San Francisco   \n",
      "1  NCT05882916                            Sue Napierala   \n",
      "2  NCT05862857  University of California, San Francisco   \n",
      "3  NCT05845619  University of California, San Francisco   \n",
      "4  NCT05842122            Fred Hutchinson Cancer Center   \n",
      "\n",
      "                             InterventionDescription  \\\n",
      "0  The Healthy Living Intervention (HLI) is a bri...   \n",
      "1  Oral fluid-based HIV self-test kits for second...   \n",
      "2  Patrons and employees of drinking venues that ...   \n",
      "3  The pilot intervention will include the follow...   \n",
      "4  Services delivered: 1) behavioral HIV risk ass...   \n",
      "\n",
      "  BaselineMeasurePopulationDescription  \\\n",
      "0                                  NaN   \n",
      "1                                  NaN   \n",
      "2                                  NaN   \n",
      "3                                  NaN   \n",
      "4                                  NaN   \n",
      "\n",
      "                  DesignInterventionModelDescription  \\\n",
      "0                                                NaN   \n",
      "1                           Cluster-randomized trial   \n",
      "2                                                NaN   \n",
      "3  Pilot study participants will be compared to h...   \n",
      "4  The study is a cluster-randomized control tria...   \n",
      "\n",
      "                    InterventionName  \\\n",
      "0  Healthy Living Intervention (HLI)   \n",
      "1                   HIV self-testing   \n",
      "2           HIV-focused mobilization   \n",
      "3      Enhanced virologic monitoring   \n",
      "4        Pharmacy PrEP/PEP for a fee   \n",
      "\n",
      "                           PrimaryOutcomeDescription  \n",
      "0  The proportion of time during the 48 weeks aft...  \n",
      "1  The count of adult men from intervention versu...  \n",
      "2  The proportion of HIV-negative adults, receivi...  \n",
      "3  Plasma HIV RNA <50 copies/mL. Data will be col...  \n",
      "4  Number of participants that initiated (i.e., w...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv ('./clinical_trial_data.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     NCTNumber                            InterventionDescription\n",
      "0  NCT06036238  The Healthy Living Intervention (HLI) is a bri...\n",
      "1  NCT05882916  Oral fluid-based HIV self-test kits for second...\n",
      "2  NCT05862857  Patrons and employees of drinking venues that ...\n",
      "3  NCT05845619  The pilot intervention will include the follow...\n",
      "4  NCT05842122  Services delivered: 1) behavioral HIV risk ass...\n"
     ]
    }
   ],
   "source": [
    "df_description = df[['NCTNumber','InterventionDescription']]\n",
    "print(df_description.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_description.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCTNumber                   0\n",
       "InterventionDescription    12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_description.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 2)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_description_clean = df_description.dropna()\n",
    "df_description_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 148 entries, 0 to 159\n",
      "Data columns (total 2 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   NCTNumber                148 non-null    object\n",
      " 1   InterventionDescription  148 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_description_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 148 entries, 0 to 159\n",
      "Data columns (total 2 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   NCTNumber                148 non-null    object\n",
      " 1   InterventionDescription  148 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_description_clean.loc[:,'InterventionDescription'] = df_description_clean['InterventionDescription'].astype('str')\n",
    "df_description_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identifying Noise through Regular expression\n",
    "\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity (text, min_len=10):\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        #return bool(RE_SUSPICIOUS.search(text))\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     NCTNumber                            InterventionDescription  impurity\n",
      "0  NCT06036238  The Healthy Living Intervention (HLI) is a bri...  0.000000\n",
      "1  NCT05882916  Oral fluid-based HIV self-test kits for second...  0.000000\n",
      "2  NCT05862857  Patrons and employees of drinking venues that ...  0.000000\n",
      "3  NCT05845619  The pilot intervention will include the follow...  0.001138\n",
      "4  NCT05842122  Services delivered: 1) behavioral HIV risk ass...  0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11068\\4134653321.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_description_clean.loc[:,'impurity'] = df_description_clean.loc[:,'InterventionDescription'].apply(impurity,min_len=10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InterventionDescription</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tu'Washindi consists of 3 primary components:\\...</td>\n",
       "      <td>0.010142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>When a participant comes for a regular clinic ...</td>\n",
       "      <td>0.005650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Economic incentives are given to the study par...</td>\n",
       "      <td>0.004695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               InterventionDescription  impurity\n",
       "8    Tu'Washindi consists of 3 primary components:\\...  0.010142\n",
       "126  When a participant comes for a regular clinic ...  0.005650\n",
       "128  Economic incentives are given to the study par...  0.004695"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_description_clean.loc[:,'impurity'] = df_description_clean.loc[:,'InterventionDescription'].apply(impurity,min_len=10)\n",
    "print(df_description_clean.head())\n",
    "\n",
    "\n",
    "df_description_clean[['InterventionDescription', 'impurity']].sort_values(by='impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # remove brackets\n",
    "    text = re.sub(r'[:;()]', '', text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r'[0-9]','', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     NCTNumber                            InterventionDescription  impurity  \\\n",
      "0  NCT06036238  The Healthy Living Intervention (HLI) is a bri...  0.000000   \n",
      "1  NCT05882916  Oral fluid-based HIV self-test kits for second...  0.000000   \n",
      "2  NCT05862857  Patrons and employees of drinking venues that ...  0.000000   \n",
      "3  NCT05845619  The pilot intervention will include the follow...  0.001138   \n",
      "4  NCT05842122  Services delivered: 1) behavioral HIV risk ass...  0.000000   \n",
      "\n",
      "                                          clean_text  \n",
      "0  The Healthy Living Intervention HLI is a brief...  \n",
      "1  Oral fluid-based HIV self-test kits for second...  \n",
      "2  Patrons and employees of drinking venues that ...  \n",
      "3  The pilot intervention will include the follow...  \n",
      "4  Services delivered  behavioral HIV risk assess...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 148 entries, 0 to 159\n",
      "Data columns (total 4 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   NCTNumber                148 non-null    object \n",
      " 1   InterventionDescription  148 non-null    object \n",
      " 2   impurity                 148 non-null    float64\n",
      " 3   clean_text               148 non-null    object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 5.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_description_clean.loc[:,'clean_text'] = df_description_clean.loc[:,'InterventionDescription'].map(clean)\n",
    "print(df_description_clean.head())\n",
    "df_description_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_description_clean.to_csv(\"Intervention.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'go', 'our', 'just', 'during', 'where', 'also', 'being', 'i', 'eleven', 'across', 'your', 'always', '’ve', 'own', 'herein', 'seemed', 'do', 'amount', 'beforehand', 'either', 'eight', 'both', 'fifty', '’m', 'call', 'onto', 'my', 'so', 'sometimes', 'then', 'by', 'top', 'very', 'for', 'take', 'otherwise', 'somehow', 'did', 'it', 'yet', 'using', 'everywhere', 'with', 'please', \"'d\", 'show', 'there', 'mostly', 'yours', 'done', 'something', 'per', 'them', 'put', 'together', 'twelve', '‘re', 'him', 'me', 'each', 'due', 'was', 'front', 'would', 'whether', 'n‘t', 'nevertheless', 'her', 'among', 'part', 'as', 'several', 'serious', 'about', 'himself', 'everything', 'ten', 'thereafter', 'thence', 'various', \"'ve\", 'myself', 'this', 'have', 'hereby', 'any', 'we', 'beyond', 'sixty', 'am', 'afterwards', 'will', 'therefore', 'almost', 'three', 'even', 'be', 'too', 'or', 'most', 'must', 'until', 'only', 'they', 'below', 'make', 'third', 'such', 'thereby', 'however', 'mine', 'which', 'in', 'really', 'every', 'whereas', 'sometime', 'first', 'four', 'except', 'least', 'anything', 'may', 'that', 'whereafter', 'say', 'towards', 'who', 'many', 'off', 'unless', 'whatever', 'amongst', 'nowhere', 'of', 'others', 'hereupon', 'cannot', \"'re\", 'n’t', 'forty', 'can', 'alone', 'indeed', 'she', 'and', 'one', 'give', '‘ve', 'again', 'his', 'between', 'a', 'when', \"'s\", 'whole', 'full', 'does', 'throughout', 'ca', 'after', 'nor', 'he', 'out', \"'ll\", 'hence', 'elsewhere', 'hundred', 'whom', 'doing', 'perhaps', 'empty', 'two', 'the', 'anyway', 'hereafter', 'nine', 'someone', 'is', 'while', 'against', '‘ll', 'could', 'not', 'some', 'side', 'whenever', 'on', 'seem', 'become', 'whereupon', 'already', 'us', 'here', 'those', 'few', 'over', 'because', \"n't\", 'up', 'through', 'are', 'down', 'before', 'no', 'keep', 'wherein', 'noone', 'well', 'everyone', 'via', 'herself', 'next', 'nobody', 'seems', 'though', '’s', 'upon', 'thus', '‘s', 'becomes', 'still', 'more', 'nothing', 'since', 'within', 'thereupon', 'move', 'somewhere', 'six', 'should', 'an', 'these', 'fifteen', 'back', 'quite', 'last', 'around', 'further', 'formerly', 'yourself', 'along', '‘m', 'became', 'whence', 'at', 'whoever', 'how', 'under', 'their', 'been', 'rather', 'wherever', '‘d', 'ourselves', 'themselves', 'get', 'therein', 'all', 'anywhere', 're', 'without', '’re', 'although', 'now', 'once', 'five', 'whereby', 'another', 'else', 'hers', 'moreover', 'whither', 'yourselves', 'whose', 'into', 'much', 'above', 'other', 'beside', 'to', 'made', 'why', 'namely', 'seeming', \"'m\", 'neither', 'bottom', 'but', 'anyhow', 'might', 'none', 'used', 'former', 'if', 'latter', 'were', 'name', 'its', 'behind', 'regarding', 'never', 'latterly', 'same', 'has', 'toward', 'anyone', 'less', 'itself', 'than', 'often', 'see', '’d', 'enough', '’ll', 'had', 'thru', 'ever', 'meanwhile', 'besides', 'you', 'twenty', 'from', 'ours', 'becoming', 'what'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m count_para_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39mstopwords, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m count_para_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mcount_para_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_description_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2134\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2135\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2136\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2137\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2138\u001b[0m )\n\u001b[1;32m-> 2139\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1145\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1141\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1142\u001b[0m )\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1145\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:638\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 638\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m     )\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'go', 'our', 'just', 'during', 'where', 'also', 'being', 'i', 'eleven', 'across', 'your', 'always', '’ve', 'own', 'herein', 'seemed', 'do', 'amount', 'beforehand', 'either', 'eight', 'both', 'fifty', '’m', 'call', 'onto', 'my', 'so', 'sometimes', 'then', 'by', 'top', 'very', 'for', 'take', 'otherwise', 'somehow', 'did', 'it', 'yet', 'using', 'everywhere', 'with', 'please', \"'d\", 'show', 'there', 'mostly', 'yours', 'done', 'something', 'per', 'them', 'put', 'together', 'twelve', '‘re', 'him', 'me', 'each', 'due', 'was', 'front', 'would', 'whether', 'n‘t', 'nevertheless', 'her', 'among', 'part', 'as', 'several', 'serious', 'about', 'himself', 'everything', 'ten', 'thereafter', 'thence', 'various', \"'ve\", 'myself', 'this', 'have', 'hereby', 'any', 'we', 'beyond', 'sixty', 'am', 'afterwards', 'will', 'therefore', 'almost', 'three', 'even', 'be', 'too', 'or', 'most', 'must', 'until', 'only', 'they', 'below', 'make', 'third', 'such', 'thereby', 'however', 'mine', 'which', 'in', 'really', 'every', 'whereas', 'sometime', 'first', 'four', 'except', 'least', 'anything', 'may', 'that', 'whereafter', 'say', 'towards', 'who', 'many', 'off', 'unless', 'whatever', 'amongst', 'nowhere', 'of', 'others', 'hereupon', 'cannot', \"'re\", 'n’t', 'forty', 'can', 'alone', 'indeed', 'she', 'and', 'one', 'give', '‘ve', 'again', 'his', 'between', 'a', 'when', \"'s\", 'whole', 'full', 'does', 'throughout', 'ca', 'after', 'nor', 'he', 'out', \"'ll\", 'hence', 'elsewhere', 'hundred', 'whom', 'doing', 'perhaps', 'empty', 'two', 'the', 'anyway', 'hereafter', 'nine', 'someone', 'is', 'while', 'against', '‘ll', 'could', 'not', 'some', 'side', 'whenever', 'on', 'seem', 'become', 'whereupon', 'already', 'us', 'here', 'those', 'few', 'over', 'because', \"n't\", 'up', 'through', 'are', 'down', 'before', 'no', 'keep', 'wherein', 'noone', 'well', 'everyone', 'via', 'herself', 'next', 'nobody', 'seems', 'though', '’s', 'upon', 'thus', '‘s', 'becomes', 'still', 'more', 'nothing', 'since', 'within', 'thereupon', 'move', 'somewhere', 'six', 'should', 'an', 'these', 'fifteen', 'back', 'quite', 'last', 'around', 'further', 'formerly', 'yourself', 'along', '‘m', 'became', 'whence', 'at', 'whoever', 'how', 'under', 'their', 'been', 'rather', 'wherever', '‘d', 'ourselves', 'themselves', 'get', 'therein', 'all', 'anywhere', 're', 'without', '’re', 'although', 'now', 'once', 'five', 'whereby', 'another', 'else', 'hers', 'moreover', 'whither', 'yourselves', 'whose', 'into', 'much', 'above', 'other', 'beside', 'to', 'made', 'why', 'namely', 'seeming', \"'m\", 'neither', 'bottom', 'but', 'anyhow', 'might', 'none', 'used', 'former', 'if', 'latter', 'were', 'name', 'its', 'behind', 'regarding', 'never', 'latterly', 'same', 'has', 'toward', 'anyone', 'less', 'itself', 'than', 'often', 'see', '’d', 'enough', '’ll', 'had', 'thru', 'ever', 'meanwhile', 'besides', 'you', 'twenty', 'from', 'ours', 'becoming', 'what'} instead."
     ]
    }
   ],
   "source": [
    "count_para_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5,max_df=0.7)\n",
    "\n",
    "count_para_vectors = count_para_vectorizer.fit_transform(df_description_clean[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
